{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc5e3f93",
   "metadata": {},
   "source": [
    "# Objective: \n",
    "\n",
    "In order to enhance VADER dictionary, we are trying to include the words from LM dictionary. \n",
    "\n",
    "## Methodolgy :\n",
    "\n",
    "1. Find the VADER dictionary \n",
    "\n",
    "2. Find the LM dictionary\n",
    "\n",
    "3. Find common words, exclusive words. \n",
    "\n",
    "4. Do analysis on these two dictionaries as you go through. \n",
    "\n",
    "5. Create a final dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8938a8",
   "metadata": {},
   "source": [
    "Found the GITHub link for VADER:\n",
    "\n",
    "\n",
    "https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "393ed417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\Users\\sreek\\Documents\\1. Praxis\\3. Term 3\\3. CAPP- New\\My  Work\\Dictionaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6d1d9a",
   "metadata": {},
   "source": [
    "vader_lexicon.txt\n",
    "\n",
    "\n",
    "FORMAT: the file is tab delimited with TOKEN, MEAN-SENTIMENT-RATING, STANDARD DEVIATION, and RAW-HUMAN-SENTIMENT-RATINGS\n",
    "NOTE: The current algorithm makes immediate use of the first two elements (token and mean valence). The final two elements (SD and raw ratings) are provided for rigor. For example, if you want to follow the same rigorous process that we used for the study, you should find 10 independent humans to evaluate/rate each new token you want to add to the lexicon, make sure the standard deviation doesn't exceed 2.5, and take the average rating for the valence. This will keep the file consistent.\n",
    "\n",
    "DESCRIPTION: Empirically validated by multiple independent human judges, VADER incorporates a \"gold-standard\" sentiment lexicon that is especially attuned to microblog-like contexts.\n",
    "\n",
    "The VADER sentiment lexicon is sensitive both the polarity and the intensity of sentiments expressed in social media contexts, and is also generally applicable to sentiment analysis in other domains.\n",
    "\n",
    "Sentiment ratings from 10 independent human raters (all pre-screened, trained, and quality checked for optimal inter-rater reliability). Over 9,000 token features were rated on a scale from \"[–4] Extremely Negative\" to \"[4] Extremely Positive\", with allowance for \"[0] Neutral (or Neither, N/A)\". \n",
    "\n",
    "We kept every lexical feature that had a non-zero mean rating, and whose standard deviation was less than 2.5 as determined by the aggregate of those ten independent raters. This left us with just over 7,500 lexical features with validated valence scores that indicated both the sentiment polarity (positive/negative), and the sentiment intensity on a scale from –4 to +4. \n",
    "\n",
    "For example, the word \"okay\" has a positive valence of 0.9, \"good\" is 1.9, and \"great\" is 3.1, whereas \"horrible\" is –2.5, the frowning emoticon :( is –2.2, and \"sucks\" and it's slang derivative \"sux\" are both –1.5.\n",
    "\n",
    "Manually creating (much less, validating) a comprehensive sentiment lexicon is a labor intensive and sometimes error prone process, so it is no wonder that many opinion mining researchers and practitioners rely so heavily on existing lexicons as primary resources. \n",
    "\n",
    "We are pleased to offer ours as a new resource. We began by constructing a list inspired by examining existing well-established sentiment word-banks (LIWC, ANEW, and GI). To this, we next incorporate numerous lexical features common to sentiment expression in microblogs, including:\n",
    "\n",
    "a full list of Western-style emoticons, for example, :-) denotes a smiley face and generally indicates positive sentiment\n",
    "sentiment-related acronyms and initialisms (e.g., LOL and WTF are both examples of sentiment-laden initialisms)\n",
    "commonly used slang with sentiment value (e.g., nah, meh and giggly).\n",
    "\n",
    "We empirically confirmed the general applicability of each feature candidate to sentiment expressions using a wisdom-of-the-crowd (WotC) approach (Surowiecki, 2004) to acquire a valid point estimate for the sentiment valence (polarity & intensity) of each context-free candidate feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a30eb505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1221bc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_sentiment = pd.read_csv(\"Vaderdict.csv\",encoding = \"cp1252\") # Had to give encoding to avoid unicodec error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cf899d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7520"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vader_sentiment.Token) # There are 7520 words including the emoticons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0e2d4098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Mean Sentiment Rating</th>\n",
       "      <th>Standard Deviation</th>\n",
       "      <th>Raw Human Sentiment Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$:</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.80623</td>\n",
       "      <td>[-1, -1, -1, -1, -3, -1, -3, -1, -2, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>%)</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>1.01980</td>\n",
       "      <td>[-1, 0, -1, 0, 0, -2, -1, 2, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>%-)</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>1.43178</td>\n",
       "      <td>[-2, 0, -2, -2, -1, 2, -2, -3, -2, -3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&amp;-:</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>1.42829</td>\n",
       "      <td>[-3, -1, 0, 0, -1, -1, -1, 2, -1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&amp;:</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.64031</td>\n",
       "      <td>[0, -1, -1, -1, 1, -1, -1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Token  Mean Sentiment Rating  Standard Deviation  \\\n",
       "0    $:                   -1.5             0.80623   \n",
       "1    %)                   -0.4             1.01980   \n",
       "2   %-)                   -1.5             1.43178   \n",
       "3   &-:                   -0.4             1.42829   \n",
       "4    &:                   -0.7             0.64031   \n",
       "\n",
       "                Raw Human Sentiment Ratings  \n",
       "0  [-1, -1, -1, -1, -3, -1, -3, -1, -2, -1]  \n",
       "1       [-1, 0, -1, 0, 0, -2, -1, 2, -1, 0]  \n",
       "2    [-2, 0, -2, -2, -1, 2, -2, -3, -2, -3]  \n",
       "3      [-3, -1, 0, 0, -1, -1, -1, 2, -1, 2]  \n",
       "4    [0, -1, -1, -1, 1, -1, -1, -1, -1, -1]  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader_sentiment.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c334f4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5       ( '}{' )\n",
       "7           ('-:\n",
       "8            (':\n",
       "9           ((-:\n",
       "10            (*\n",
       "          ...   \n",
       "7503     zealous\n",
       "7504          {:\n",
       "7510        |;-)\n",
       "7517         }:)\n",
       "7519        }:-)\n",
       "Name: Token, Length: 3347, dtype: object"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of positive words in vader \n",
    "vader_sentiment[vader_sentiment[\"Mean Sentiment Rating\"]>0].Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1c1d74bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Mean Sentiment Rating</th>\n",
       "      <th>Standard Deviation</th>\n",
       "      <th>Raw Human Sentiment Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$:</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.80623</td>\n",
       "      <td>[-1, -1, -1, -1, -3, -1, -3, -1, -2, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>%)</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>1.01980</td>\n",
       "      <td>[-1, 0, -1, 0, 0, -2, -1, 2, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>%-)</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>1.43178</td>\n",
       "      <td>[-2, 0, -2, -2, -1, 2, -2, -3, -2, -3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&amp;-:</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>1.42829</td>\n",
       "      <td>[-3, -1, 0, 0, -1, -1, -1, 2, -1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&amp;:</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.64031</td>\n",
       "      <td>[0, -1, -1, -1, 1, -1, -1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7513</th>\n",
       "      <td>|o:</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.53852</td>\n",
       "      <td>[-1, 0, -1, -2, -1, 0, -1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7514</th>\n",
       "      <td>||-:</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>0.45826</td>\n",
       "      <td>[-2, -2, -2, -3, -3, -3, -2, -2, -2, -2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7515</th>\n",
       "      <td>}:</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>0.83066</td>\n",
       "      <td>[-1, -1, -3, -2, -3, -2, -2, -1, -3, -3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7516</th>\n",
       "      <td>}:(</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.63246</td>\n",
       "      <td>[-3, -1, -2, -1, -3, -2, -2, -2, -2, -2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7518</th>\n",
       "      <td>}:-(</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>0.70000</td>\n",
       "      <td>[-2, -1, -2, -2, -2, -4, -2, -2, -2, -2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4173 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Token  Mean Sentiment Rating  Standard Deviation  \\\n",
       "0       $:                   -1.5             0.80623   \n",
       "1       %)                   -0.4             1.01980   \n",
       "2      %-)                   -1.5             1.43178   \n",
       "3      &-:                   -0.4             1.42829   \n",
       "4       &:                   -0.7             0.64031   \n",
       "...    ...                    ...                 ...   \n",
       "7513   |o:                   -0.9             0.53852   \n",
       "7514  ||-:                   -2.3             0.45826   \n",
       "7515    }:                   -2.1             0.83066   \n",
       "7516   }:(                   -2.0             0.63246   \n",
       "7518  }:-(                   -2.1             0.70000   \n",
       "\n",
       "                   Raw Human Sentiment Ratings  \n",
       "0     [-1, -1, -1, -1, -3, -1, -3, -1, -2, -1]  \n",
       "1          [-1, 0, -1, 0, 0, -2, -1, 2, -1, 0]  \n",
       "2       [-2, 0, -2, -2, -1, 2, -2, -3, -2, -3]  \n",
       "3         [-3, -1, 0, 0, -1, -1, -1, 2, -1, 2]  \n",
       "4       [0, -1, -1, -1, 1, -1, -1, -1, -1, -1]  \n",
       "...                                        ...  \n",
       "7513    [-1, 0, -1, -2, -1, 0, -1, -1, -1, -1]  \n",
       "7514  [-2, -2, -2, -3, -3, -3, -2, -2, -2, -2]  \n",
       "7515  [-1, -1, -3, -2, -3, -2, -2, -1, -3, -3]  \n",
       "7516  [-3, -1, -2, -1, -3, -2, -2, -2, -2, -2]  \n",
       "7518  [-2, -1, -2, -2, -2, -4, -2, -2, -2, -2]  \n",
       "\n",
       "[4173 rows x 4 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader_sentiment[vader_sentiment[\"Mean Sentiment Rating\"]<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d289d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will try to load the lm dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "117be8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\sreek\\Documents\\1. Praxis\\3. Term 3\\3. CAPP- New\\Data\\4.Dictionaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ed38aa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_dict = pd.read_csv(\"Loughran-McDonald_MasterDictionary_1993-2021.csv\") # Need to extratc lm dict from this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "47d3555e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Seq_num</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Word Proportion</th>\n",
       "      <th>Average Proportion</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Doc Count</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Uncertainty</th>\n",
       "      <th>Litigious</th>\n",
       "      <th>Strong_Modal</th>\n",
       "      <th>Weak_Modal</th>\n",
       "      <th>Constraining</th>\n",
       "      <th>Syllables</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AARDVARK</td>\n",
       "      <td>1</td>\n",
       "      <td>354</td>\n",
       "      <td>1.550080e-08</td>\n",
       "      <td>1.422600e-08</td>\n",
       "      <td>3.815486e-06</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AARDVARKS</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.313627e-10</td>\n",
       "      <td>8.653817e-12</td>\n",
       "      <td>9.241714e-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABACI</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3.940882e-10</td>\n",
       "      <td>1.169679e-10</td>\n",
       "      <td>5.290465e-08</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABACK</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>1.269840e-09</td>\n",
       "      <td>6.654735e-10</td>\n",
       "      <td>1.595100e-07</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABACUS</td>\n",
       "      <td>5</td>\n",
       "      <td>8570</td>\n",
       "      <td>3.752595e-07</td>\n",
       "      <td>3.809464e-07</td>\n",
       "      <td>3.529356e-05</td>\n",
       "      <td>1108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86526</th>\n",
       "      <td>ZYGOTE</td>\n",
       "      <td>86529</td>\n",
       "      <td>50</td>\n",
       "      <td>2.189379e-09</td>\n",
       "      <td>8.729336e-10</td>\n",
       "      <td>1.886011e-07</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86527</th>\n",
       "      <td>ZYGOTES</td>\n",
       "      <td>86530</td>\n",
       "      <td>1</td>\n",
       "      <td>4.378757e-11</td>\n",
       "      <td>1.809516e-11</td>\n",
       "      <td>1.932446e-08</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86528</th>\n",
       "      <td>ZYGOTIC</td>\n",
       "      <td>86531</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86529</th>\n",
       "      <td>ZYMURGIES</td>\n",
       "      <td>86532</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86530</th>\n",
       "      <td>ZYMURGY</td>\n",
       "      <td>86533</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86531 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  Seq_num  Word Count  Word Proportion  Average Proportion  \\\n",
       "0       AARDVARK        1         354     1.550080e-08        1.422600e-08   \n",
       "1      AARDVARKS        2           3     1.313627e-10        8.653817e-12   \n",
       "2          ABACI        3           9     3.940882e-10        1.169679e-10   \n",
       "3          ABACK        4          29     1.269840e-09        6.654735e-10   \n",
       "4         ABACUS        5        8570     3.752595e-07        3.809464e-07   \n",
       "...          ...      ...         ...              ...                 ...   \n",
       "86526     ZYGOTE    86529          50     2.189379e-09        8.729336e-10   \n",
       "86527    ZYGOTES    86530           1     4.378757e-11        1.809516e-11   \n",
       "86528    ZYGOTIC    86531           0     0.000000e+00        0.000000e+00   \n",
       "86529  ZYMURGIES    86532           0     0.000000e+00        0.000000e+00   \n",
       "86530    ZYMURGY    86533           0     0.000000e+00        0.000000e+00   \n",
       "\n",
       "            Std Dev  Doc Count  Negative  Positive  Uncertainty  Litigious  \\\n",
       "0      3.815486e-06         99         0         0            0          0   \n",
       "1      9.241714e-09          1         0         0            0          0   \n",
       "2      5.290465e-08          7         0         0            0          0   \n",
       "3      1.595100e-07         28         0         0            0          0   \n",
       "4      3.529356e-05       1108         0         0            0          0   \n",
       "...             ...        ...       ...       ...          ...        ...   \n",
       "86526  1.886011e-07         35         0         0            0          0   \n",
       "86527  1.932446e-08          1         0         0            0          0   \n",
       "86528  0.000000e+00          0         0         0            0          0   \n",
       "86529  0.000000e+00          0         0         0            0          0   \n",
       "86530  0.000000e+00          0         0         0            0          0   \n",
       "\n",
       "       Strong_Modal  Weak_Modal  Constraining  Syllables     Source  \n",
       "0                 0           0             0          2  12of12inf  \n",
       "1                 0           0             0          2  12of12inf  \n",
       "2                 0           0             0          3  12of12inf  \n",
       "3                 0           0             0          2  12of12inf  \n",
       "4                 0           0             0          3  12of12inf  \n",
       "...             ...         ...           ...        ...        ...  \n",
       "86526             0           0             0          2  12of12inf  \n",
       "86527             0           0             0          2  12of12inf  \n",
       "86528             0           0             0          3  12of12inf  \n",
       "86529             0           0             0          3  12of12inf  \n",
       "86530             0           0             0          3  12of12inf  \n",
       "\n",
       "[86531 rows x 16 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dict"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc420bcc",
   "metadata": {},
   "source": [
    "# Rule is in columns: Negative\tPositive\tUncertainty\tLitigious\tStrong_Modal\tWeak_Modal\tConstraining\n",
    "#>0 meaning they are in the lm dict \n",
    "# Extracting those rows as a seperate dataframe is the task now. \n",
    "# Method: Looking to subset the data frame and extract rows. \n",
    "\n",
    "lm_dict = Havard_dict[(Havard_dict.Negative>0)|(Havard_dict.Positive>0)|(Havard_dict.Uncertainty>0)|(Havard_dict.Litigious>0)|(Havard_dict.Strong_Modal>0)|\n",
    "            (Havard_dict.Weak_Modal>0)|(Havard_dict.Constraining>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1c710da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_words = list(lm_dict[lm_dict.Negative>0].Word.str.lower())+list(lm_dict[lm_dict.Positive>0].Word.str.lower())+list(lm_dict[lm_dict.Uncertainty>0].Word.str.lower())+list(lm_dict[lm_dict.Litigious>0].Word.str.lower())+list(lm_dict[lm_dict.Strong_Modal>0].Word.str.lower())+list(lm_dict[lm_dict.Weak_Modal>0].Word.str.lower())+list(lm_dict[lm_dict.Constraining>0].Word.str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7b27b1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4122"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a1f4f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_words = list(vader_sentiment.Token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8029641f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7520"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vader_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a87e0619",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now, lets check if there are any common words between lm and vader \n",
    "## Listing those words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f0ae1",
   "metadata": {},
   "source": [
    "### Lm vs Vader "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da53dd48",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Intersection words \n",
    "\n",
    "#x.intersection(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f0e03cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_set = set(lm_words)\n",
    "vader_set = set(vader_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "24c080a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 7520)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vader_set),len(vader_words) # 20 words are dulicate in vader_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d6c5546a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':-p', '#NAME?', '#NAME?', '#NAME?', '#NAME?', '#NAME?', '#NAME?', 'd:', 'd=', 'fav', 'lmao', 'lol', 'muah', 'o.o', 'ok', 'sob', 'x-d', 'x-p', 'xd', 'xp']\n"
     ]
    }
   ],
   "source": [
    "# Printing duplicate words \n",
    "newlist = [] # empty list to hold unique elements from the list\n",
    "duplist = [] # empty list to hold the duplicate elements from the list\n",
    "for i in vader_words:\n",
    "    if i not in newlist:\n",
    "        newlist.append(i)\n",
    "    else:\n",
    "        duplist.append(i) # this \n",
    "print(duplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "6cd90c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3859, 4122)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_set),len(lm_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "e43530a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Difference \n",
    "4122-3859\n",
    "# 263 non unique words are present here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "0e46b9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newlist = [] # empty list to hold unique elements from the list\n",
    "duplist = [] # empty list to hold the duplicate elements from the list\n",
    "for i in lm_words:\n",
    "    if i not in newlist:\n",
    "        newlist.append(i)\n",
    "    else:\n",
    "        duplist.append(i) # th\n",
    "len(duplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8047e78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "930"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding common words:\n",
    "len(lm_set.intersection(vader_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "285c3986",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = lm_set.intersection(vader_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "c8cbdf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\sreek\\Documents\\1. Praxis\\3. Term 3\\3. CAPP- New\\My  Work\\Dictionaries\")\n",
    "pd.DataFrame(lm_set.intersection(vader_set)).to_csv(\"intersection.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1412bbce",
   "metadata": {},
   "source": [
    "## Positive Words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad6d2f9",
   "metadata": {},
   "source": [
    "### Intersection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "122e3f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_p_set = vader_sentiment[vader_sentiment[\"Mean Sentiment Rating\"]>0].Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "e4958ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "newlist = [] # empty list to hold unique elements from the list\n",
    "duplist = [] # empty list to hold the duplicate elements from the list\n",
    "for i in v_p_set:\n",
    "    if i not in newlist:\n",
    "        newlist.append(i)\n",
    "    else:\n",
    "        duplist.append(i) # this method catches the first duplicate entries, and appends them to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "aecd155a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':-p',\n",
       " '#NAME?',\n",
       " '#NAME?',\n",
       " '#NAME?',\n",
       " '#NAME?',\n",
       " 'fav',\n",
       " 'lmao',\n",
       " 'lol',\n",
       " 'muah',\n",
       " 'ok',\n",
       " 'x-d',\n",
       " 'x-p',\n",
       " 'xd',\n",
       " 'xp']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplist # Duplicated positives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "c9bd8015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lm positive word set \n",
    "lm_p_set=set(lm_dict[lm_dict.Positive>0].Word.str.lower())\n",
    "# Vader positive word set \n",
    "v_p_set = set(vader_sentiment[vader_sentiment[\"Mean Sentiment Rating\"]>0].Token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "46356056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(347, 3333)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_p_set),len((v_p_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "289a3c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Intersection words \n",
    "\n",
    "len(lm_p_set.intersection(v_p_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c0b1ef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\sreek\\Documents\\1. Praxis\\3. Term 3\\3. CAPP- New\\My  Work\\Dictionaries\")\n",
    "pd.DataFrame(lm_p_set.intersection(v_p_set)).to_csv(\"positive_intersection.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "bb575ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_p_set-v_p_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "9903eba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3154"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v_p_set-lm_p_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a174cc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3680"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "347+3333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "d23e387d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3694"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3347+347"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d161519b",
   "metadata": {},
   "source": [
    "## Negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "925a1100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lm positive word set \n",
    "lm_n_set=set(lm_dict[lm_dict.Negative>0].Word.str.lower())\n",
    "# Vader positive word set \n",
    "v_n_set = set(vader_sentiment[vader_sentiment[\"Mean Sentiment Rating\"]<0].Token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "52a05184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2345, 4170)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_n_set),len((v_n_set)) # 4173 is the list given. so 3 duplicates exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "f0b9386f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "662"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Intersection words \n",
    "len(lm_n_set.intersection(v_n_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "be9f878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\sreek\\Documents\\1. Praxis\\3. Term 3\\3. CAPP- New\\My  Work\\Dictionaries\")\n",
    "pd.DataFrame(lm_n_set.intersection(v_n_set)).to_csv(\"negative_intersection.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "9472199f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1683"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_n_set-v_n_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "7b0820e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3508"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v_n_set-lm_n_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "fea11792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6515"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2345+4170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "4c11c71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2345"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1683+662"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "d786d9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4170"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3508+662"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e9588c",
   "metadata": {},
   "source": [
    "## Positive vs Negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "e9fb1b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unmatched'}"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Intersection words \n",
    "lm_p_set.intersection(v_n_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "2216f1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acquit',\n",
       " 'acquits',\n",
       " 'acquitted',\n",
       " 'acquitting',\n",
       " 'challenge',\n",
       " 'challenges',\n",
       " 'challenging',\n",
       " 'conciliating',\n",
       " 'contend',\n",
       " 'defensive',\n",
       " 'easing',\n",
       " 'exonerate',\n",
       " 'exonerated',\n",
       " 'exonerates',\n",
       " 'exonerating',\n",
       " 'opportunistically',\n",
       " 'prevents',\n",
       " 'urgent',\n",
       " 'verdict',\n",
       " 'verdicts'}"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Intersection words \n",
    "(lm_n_set.intersection(v_p_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "6bb57d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "346"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_p_set-(v_n_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f00266",
   "metadata": {},
   "source": [
    "## Finding the polarity for common words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "8edf20ec",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abandon',\n",
       " 'abandoned',\n",
       " 'abandoning',\n",
       " 'abandonment',\n",
       " 'abandonments',\n",
       " 'abandons',\n",
       " 'absolve',\n",
       " 'absolved',\n",
       " 'absolves',\n",
       " 'absolving',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abuses',\n",
       " 'abusing',\n",
       " 'abusive',\n",
       " 'abusively',\n",
       " 'abusiveness',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidentally',\n",
       " 'accidents',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishes',\n",
       " 'accusation',\n",
       " 'accusations',\n",
       " 'accuse',\n",
       " 'accused',\n",
       " 'accuses',\n",
       " 'accusing',\n",
       " 'acquit',\n",
       " 'acquits',\n",
       " 'acquitted',\n",
       " 'acquitting',\n",
       " 'advantage',\n",
       " 'advantaged',\n",
       " 'advantageous',\n",
       " 'advantageously',\n",
       " 'advantages',\n",
       " 'adversarial',\n",
       " 'adversaries',\n",
       " 'adversary',\n",
       " 'adverse',\n",
       " 'adversely',\n",
       " 'adversities',\n",
       " 'adversity',\n",
       " 'aggravate',\n",
       " 'aggravated',\n",
       " 'aggravates',\n",
       " 'aggravating',\n",
       " 'alienation',\n",
       " 'annoy',\n",
       " 'annoyance',\n",
       " 'annoyances',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'annoys',\n",
       " 'anticipation',\n",
       " 'argue',\n",
       " 'argued',\n",
       " 'arguing',\n",
       " 'argument',\n",
       " 'argumentative',\n",
       " 'arguments',\n",
       " 'arrest',\n",
       " 'arrested',\n",
       " 'arrests',\n",
       " 'assault',\n",
       " 'assaulted',\n",
       " 'assaulting',\n",
       " 'assaults',\n",
       " 'assure',\n",
       " 'assured',\n",
       " 'assures',\n",
       " 'assuring',\n",
       " 'attractive',\n",
       " 'attractiveness',\n",
       " 'bad',\n",
       " 'bailout',\n",
       " 'bankrupt',\n",
       " 'barrier',\n",
       " 'beautiful',\n",
       " 'beautifully',\n",
       " 'beneficial',\n",
       " 'beneficially',\n",
       " 'beneficiation',\n",
       " 'benefitted',\n",
       " 'benefitting',\n",
       " 'best',\n",
       " 'better',\n",
       " 'boost',\n",
       " 'boosted',\n",
       " 'boycott',\n",
       " 'boycotted',\n",
       " 'boycotting',\n",
       " 'boycotts',\n",
       " 'bribe',\n",
       " 'brilliant',\n",
       " 'burden',\n",
       " 'burdened',\n",
       " 'burdening',\n",
       " 'burdens',\n",
       " 'burdensome',\n",
       " 'cancel',\n",
       " 'cancelled',\n",
       " 'cancelling',\n",
       " 'cancels',\n",
       " 'careless',\n",
       " 'carelessly',\n",
       " 'carelessness',\n",
       " 'catastrophe',\n",
       " 'catastrophic',\n",
       " 'cautious',\n",
       " 'challenge',\n",
       " 'challenged',\n",
       " 'challenges',\n",
       " 'challenging',\n",
       " 'charitable',\n",
       " 'clearly',\n",
       " 'coerced',\n",
       " 'collapse',\n",
       " 'collapsed',\n",
       " 'collapses',\n",
       " 'collapsing',\n",
       " 'collision',\n",
       " 'collisions',\n",
       " 'colluding',\n",
       " 'commit',\n",
       " 'commitment',\n",
       " 'commitments',\n",
       " 'commits',\n",
       " 'committed',\n",
       " 'committing',\n",
       " 'compelled',\n",
       " 'compelling',\n",
       " 'complain',\n",
       " 'complainant',\n",
       " 'complainants',\n",
       " 'complained',\n",
       " 'complaining',\n",
       " 'complains',\n",
       " 'complaint',\n",
       " 'complaints',\n",
       " 'compliment',\n",
       " 'complimentary',\n",
       " 'complimented',\n",
       " 'complimenting',\n",
       " 'compliments',\n",
       " 'conciliating',\n",
       " 'condemn',\n",
       " 'condemnation',\n",
       " 'condemned',\n",
       " 'condemns',\n",
       " 'confident',\n",
       " 'conflict',\n",
       " 'conflicting',\n",
       " 'conflicts',\n",
       " 'confront',\n",
       " 'confrontation',\n",
       " 'confrontational',\n",
       " 'confrontations',\n",
       " 'confronted',\n",
       " 'confronting',\n",
       " 'confronts',\n",
       " 'confuse',\n",
       " 'confused',\n",
       " 'confuses',\n",
       " 'confusing',\n",
       " 'confusingly',\n",
       " 'confusion',\n",
       " 'consent',\n",
       " 'consents',\n",
       " 'conspiracy',\n",
       " 'constrained',\n",
       " 'contempt',\n",
       " 'contend',\n",
       " 'contentious',\n",
       " 'contradict',\n",
       " 'contradicted',\n",
       " 'contradicting',\n",
       " 'contradiction',\n",
       " 'contradictions',\n",
       " 'contradictory',\n",
       " 'contradicts',\n",
       " 'controversial',\n",
       " 'costly',\n",
       " 'courteous',\n",
       " 'creative',\n",
       " 'creatively',\n",
       " 'creativeness',\n",
       " 'creativity',\n",
       " 'crime',\n",
       " 'criminal',\n",
       " 'criminals',\n",
       " 'crisis',\n",
       " 'criticism',\n",
       " 'criticisms',\n",
       " 'criticize',\n",
       " 'criticized',\n",
       " 'criticizes',\n",
       " 'criticizing',\n",
       " 'cut',\n",
       " 'damage',\n",
       " 'damaged',\n",
       " 'damages',\n",
       " 'damaging',\n",
       " 'danger',\n",
       " 'dangerous',\n",
       " 'dangerously',\n",
       " 'dangers',\n",
       " 'deadlock',\n",
       " 'deceit',\n",
       " 'deceitful',\n",
       " 'deceive',\n",
       " 'deceived',\n",
       " 'deceives',\n",
       " 'deceiving',\n",
       " 'deception',\n",
       " 'defeat',\n",
       " 'defeated',\n",
       " 'defeating',\n",
       " 'defeats',\n",
       " 'defect',\n",
       " 'defective',\n",
       " 'defectively',\n",
       " 'defects',\n",
       " 'defensive',\n",
       " 'defer',\n",
       " 'deficit',\n",
       " 'definitely',\n",
       " 'degradation',\n",
       " 'degradations',\n",
       " 'degrade',\n",
       " 'degraded',\n",
       " 'degrades',\n",
       " 'degrading',\n",
       " 'delay',\n",
       " 'delayed',\n",
       " 'delight',\n",
       " 'delighted',\n",
       " 'delightful',\n",
       " 'delightfully',\n",
       " 'delighting',\n",
       " 'delights',\n",
       " 'denied',\n",
       " 'denies',\n",
       " 'deny',\n",
       " 'denying',\n",
       " 'depress',\n",
       " 'depressed',\n",
       " 'depresses',\n",
       " 'depressing',\n",
       " 'deprivation',\n",
       " 'deprive',\n",
       " 'deprived',\n",
       " 'deprives',\n",
       " 'depriving',\n",
       " 'desirable',\n",
       " 'desired',\n",
       " 'destroy',\n",
       " 'destroyed',\n",
       " 'destroying',\n",
       " 'destroys',\n",
       " 'destruction',\n",
       " 'destructive',\n",
       " 'detain',\n",
       " 'detained',\n",
       " 'detention',\n",
       " 'devastate',\n",
       " 'devastated',\n",
       " 'devastating',\n",
       " 'devastation',\n",
       " 'difficult',\n",
       " 'difficulties',\n",
       " 'difficultly',\n",
       " 'difficulty',\n",
       " 'disadvantage',\n",
       " 'disadvantaged',\n",
       " 'disadvantageous',\n",
       " 'disadvantages',\n",
       " 'disagree',\n",
       " 'disagreeable',\n",
       " 'disagreed',\n",
       " 'disagreeing',\n",
       " 'disagreement',\n",
       " 'disagreements',\n",
       " 'disagrees',\n",
       " 'disappear',\n",
       " 'disappeared',\n",
       " 'disappears',\n",
       " 'disappoint',\n",
       " 'disappointed',\n",
       " 'disappointing',\n",
       " 'disappointingly',\n",
       " 'disappointment',\n",
       " 'disappointments',\n",
       " 'disappoints',\n",
       " 'disaster',\n",
       " 'disasters',\n",
       " 'disastrous',\n",
       " 'discourage',\n",
       " 'discouraged',\n",
       " 'discourages',\n",
       " 'discouraging',\n",
       " 'discredited',\n",
       " 'disgrace',\n",
       " 'dishonest',\n",
       " 'dismal',\n",
       " 'disparage',\n",
       " 'disparaged',\n",
       " 'disparages',\n",
       " 'disparaging',\n",
       " 'dispute',\n",
       " 'disputed',\n",
       " 'disputes',\n",
       " 'disputing',\n",
       " 'disqualified',\n",
       " 'disregard',\n",
       " 'disregarded',\n",
       " 'disregarding',\n",
       " 'disregards',\n",
       " 'disruption',\n",
       " 'disruptions',\n",
       " 'disruptive',\n",
       " 'dissatisfaction',\n",
       " 'dissatisfied',\n",
       " 'distort',\n",
       " 'distorted',\n",
       " 'distorting',\n",
       " 'distorts',\n",
       " 'distract',\n",
       " 'distracted',\n",
       " 'distracting',\n",
       " 'distraction',\n",
       " 'distractions',\n",
       " 'distracts',\n",
       " 'distress',\n",
       " 'distressed',\n",
       " 'disturb',\n",
       " 'disturbance',\n",
       " 'disturbances',\n",
       " 'disturbed',\n",
       " 'disturbing',\n",
       " 'disturbs',\n",
       " 'doubt',\n",
       " 'doubted',\n",
       " 'doubtful',\n",
       " 'doubts',\n",
       " 'drag',\n",
       " 'dream',\n",
       " 'dysfunction',\n",
       " 'easier',\n",
       " 'easily',\n",
       " 'easing',\n",
       " 'easy',\n",
       " 'efficiencies',\n",
       " 'efficiency',\n",
       " 'efficient',\n",
       " 'efficiently',\n",
       " 'embarrass',\n",
       " 'embarrassed',\n",
       " 'embarrasses',\n",
       " 'embarrassing',\n",
       " 'embarrassment',\n",
       " 'embarrassments',\n",
       " 'encouraged',\n",
       " 'encouragement',\n",
       " 'encourages',\n",
       " 'encouraging',\n",
       " 'enjoy',\n",
       " 'enjoyable',\n",
       " 'enjoyably',\n",
       " 'enjoyed',\n",
       " 'enjoying',\n",
       " 'enjoyment',\n",
       " 'enjoys',\n",
       " 'enthusiasm',\n",
       " 'enthusiastic',\n",
       " 'enthusiastically',\n",
       " 'erroneous',\n",
       " 'error',\n",
       " 'errors',\n",
       " 'eviction',\n",
       " 'exaggerate',\n",
       " 'exaggerated',\n",
       " 'exaggerates',\n",
       " 'exaggerating',\n",
       " 'excellence',\n",
       " 'excellent',\n",
       " 'excelling',\n",
       " 'excels',\n",
       " 'excited',\n",
       " 'excitement',\n",
       " 'exciting',\n",
       " 'exclusive',\n",
       " 'exonerate',\n",
       " 'exonerated',\n",
       " 'exonerates',\n",
       " 'exonerating',\n",
       " 'exploit',\n",
       " 'exploited',\n",
       " 'exploiting',\n",
       " 'exploits',\n",
       " 'expose',\n",
       " 'exposed',\n",
       " 'exposes',\n",
       " 'exposing',\n",
       " 'fail',\n",
       " 'failed',\n",
       " 'failing',\n",
       " 'failings',\n",
       " 'fails',\n",
       " 'failure',\n",
       " 'failures',\n",
       " 'falsified',\n",
       " 'falsify',\n",
       " 'fantastic',\n",
       " 'fatalities',\n",
       " 'fatality',\n",
       " 'fatally',\n",
       " 'fault',\n",
       " 'faulted',\n",
       " 'faults',\n",
       " 'faulty',\n",
       " 'favorable',\n",
       " 'favorably',\n",
       " 'favored',\n",
       " 'favoring',\n",
       " 'favorite',\n",
       " 'favorites',\n",
       " 'fear',\n",
       " 'fears',\n",
       " 'felonies',\n",
       " 'felony',\n",
       " 'fired',\n",
       " 'firing',\n",
       " 'flawed',\n",
       " 'forbid',\n",
       " 'forbidden',\n",
       " 'forbidding',\n",
       " 'forbids',\n",
       " 'forced',\n",
       " 'foreclosure',\n",
       " 'foreclosures',\n",
       " 'fraud',\n",
       " 'frauds',\n",
       " 'fraudulence',\n",
       " 'fraudulent',\n",
       " 'friendly',\n",
       " 'frustrate',\n",
       " 'frustrated',\n",
       " 'frustrates',\n",
       " 'frustrating',\n",
       " 'frustratingly',\n",
       " 'frustration',\n",
       " 'frustrations',\n",
       " 'gain',\n",
       " 'gained',\n",
       " 'gaining',\n",
       " 'gains',\n",
       " 'good',\n",
       " 'greatest',\n",
       " 'grievance',\n",
       " 'grievances',\n",
       " 'grossly',\n",
       " 'guilty',\n",
       " 'happiest',\n",
       " 'happily',\n",
       " 'happiness',\n",
       " 'happy',\n",
       " 'harass',\n",
       " 'harassed',\n",
       " 'harassing',\n",
       " 'harassment',\n",
       " 'hardship',\n",
       " 'harm',\n",
       " 'harmed',\n",
       " 'harmfully',\n",
       " 'harming',\n",
       " 'harms',\n",
       " 'harsh',\n",
       " 'harsher',\n",
       " 'harshest',\n",
       " 'hindrance',\n",
       " 'honor',\n",
       " 'honored',\n",
       " 'honoring',\n",
       " 'honors',\n",
       " 'hostile',\n",
       " 'hostility',\n",
       " 'hurt',\n",
       " 'hurting',\n",
       " 'ideal',\n",
       " 'ignore',\n",
       " 'ignored',\n",
       " 'ignores',\n",
       " 'ignoring',\n",
       " 'ill',\n",
       " 'illegal',\n",
       " 'immoral',\n",
       " 'impose',\n",
       " 'imposed',\n",
       " 'imposes',\n",
       " 'imposing',\n",
       " 'impress',\n",
       " 'impressed',\n",
       " 'impresses',\n",
       " 'impressing',\n",
       " 'impressive',\n",
       " 'impressively',\n",
       " 'improve',\n",
       " 'improved',\n",
       " 'improvement',\n",
       " 'improvements',\n",
       " 'improves',\n",
       " 'improving',\n",
       " 'inability',\n",
       " 'inaction',\n",
       " 'inadequacies',\n",
       " 'inadequacy',\n",
       " 'inadequate',\n",
       " 'inadequately',\n",
       " 'incapable',\n",
       " 'incapacitated',\n",
       " 'incompetence',\n",
       " 'incompetent',\n",
       " 'inconvenience',\n",
       " 'inconvenient',\n",
       " 'ineffective',\n",
       " 'ineffectively',\n",
       " 'ineffectiveness',\n",
       " 'inferior',\n",
       " 'influential',\n",
       " 'infringement',\n",
       " 'inhibit',\n",
       " 'inhibited',\n",
       " 'inhibiting',\n",
       " 'inhibits',\n",
       " 'injured',\n",
       " 'injury',\n",
       " 'innovate',\n",
       " 'innovates',\n",
       " 'innovation',\n",
       " 'innovative',\n",
       " 'insecure',\n",
       " 'insensitive',\n",
       " 'inspiration',\n",
       " 'inspirational',\n",
       " 'integrity',\n",
       " 'interrogated',\n",
       " 'interrupt',\n",
       " 'interrupted',\n",
       " 'interrupting',\n",
       " 'interruption',\n",
       " 'interruptions',\n",
       " 'interrupts',\n",
       " 'intimidation',\n",
       " 'irreversible',\n",
       " 'justice',\n",
       " 'lack',\n",
       " 'lag',\n",
       " 'lagged',\n",
       " 'lagging',\n",
       " 'lags',\n",
       " 'lawsuit',\n",
       " 'lawsuits',\n",
       " 'legal',\n",
       " 'legally',\n",
       " 'libelous',\n",
       " 'limitation',\n",
       " 'litigation',\n",
       " 'litigious',\n",
       " 'lose',\n",
       " 'loses',\n",
       " 'losing',\n",
       " 'loss',\n",
       " 'losses',\n",
       " 'lost',\n",
       " 'lowest',\n",
       " 'loyal',\n",
       " 'lying',\n",
       " 'mandatory',\n",
       " 'manipulated',\n",
       " 'manipulating',\n",
       " 'manipulation',\n",
       " 'meritorious',\n",
       " 'mischief',\n",
       " 'misinformation',\n",
       " 'misinformed',\n",
       " 'misinterpreted',\n",
       " 'misleading',\n",
       " 'misrepresentation',\n",
       " 'miss',\n",
       " 'missed',\n",
       " 'misses',\n",
       " 'mistake',\n",
       " 'mistaken',\n",
       " 'mistakenly',\n",
       " 'mistakes',\n",
       " 'mistaking',\n",
       " 'misunderstand',\n",
       " 'misunderstanding',\n",
       " 'misunderstood',\n",
       " 'monopolize',\n",
       " 'monopolized',\n",
       " 'monopolizes',\n",
       " 'monopolizing',\n",
       " 'negative',\n",
       " 'neglect',\n",
       " 'neglected',\n",
       " 'neglectful',\n",
       " 'neglecting',\n",
       " 'neglects',\n",
       " 'obscene',\n",
       " 'obsolete',\n",
       " 'obstacle',\n",
       " 'obstacles',\n",
       " 'offence',\n",
       " 'offences',\n",
       " 'offend',\n",
       " 'offended',\n",
       " 'offender',\n",
       " 'offenders',\n",
       " 'offending',\n",
       " 'offends',\n",
       " 'offense',\n",
       " 'opportunistic',\n",
       " 'opportunistically',\n",
       " 'opportunities',\n",
       " 'opportunity',\n",
       " 'optimistic',\n",
       " 'overload',\n",
       " 'overlooked',\n",
       " 'overstatement',\n",
       " 'overstatements',\n",
       " 'panic',\n",
       " 'panics',\n",
       " 'penalty',\n",
       " 'perfect',\n",
       " 'perfected',\n",
       " 'perfectly',\n",
       " 'perfects',\n",
       " 'peril',\n",
       " 'perjury',\n",
       " 'petty',\n",
       " 'pleasant',\n",
       " 'pleasantly',\n",
       " 'pleased',\n",
       " 'pleasure',\n",
       " 'poor',\n",
       " 'popular',\n",
       " 'popularity',\n",
       " 'positive',\n",
       " 'positively',\n",
       " 'postpone',\n",
       " 'postponed',\n",
       " 'postpones',\n",
       " 'postponing',\n",
       " 'prejudice',\n",
       " 'prejudiced',\n",
       " 'prejudices',\n",
       " 'prejudicial',\n",
       " 'prejudicing',\n",
       " 'prevent',\n",
       " 'prevented',\n",
       " 'preventing',\n",
       " 'prevents',\n",
       " 'proactive',\n",
       " 'problem',\n",
       " 'problematic',\n",
       " 'problematical',\n",
       " 'problems',\n",
       " 'profitability',\n",
       " 'profitable',\n",
       " 'profitably',\n",
       " 'progress',\n",
       " 'prosecute',\n",
       " 'prosecuted',\n",
       " 'prosecutes',\n",
       " 'prosecution',\n",
       " 'prosperous',\n",
       " 'protest',\n",
       " 'protested',\n",
       " 'protesters',\n",
       " 'protesting',\n",
       " 'protests',\n",
       " 'provoke',\n",
       " 'provoked',\n",
       " 'provokes',\n",
       " 'provoking',\n",
       " 'punishable',\n",
       " 'punished',\n",
       " 'punishes',\n",
       " 'punishing',\n",
       " 'punishment',\n",
       " 'punishments',\n",
       " 'punitive',\n",
       " 'questionable',\n",
       " 'questioned',\n",
       " 'questioning',\n",
       " 'recession',\n",
       " 'reckless',\n",
       " 'refuse',\n",
       " 'refused',\n",
       " 'refusing',\n",
       " 'reject',\n",
       " 'rejected',\n",
       " 'rejecting',\n",
       " 'rejection',\n",
       " 'rejections',\n",
       " 'rejects',\n",
       " 'reluctance',\n",
       " 'reluctant',\n",
       " 'resign',\n",
       " 'resignation',\n",
       " 'resignations',\n",
       " 'resigned',\n",
       " 'resigning',\n",
       " 'resigns',\n",
       " 'resolve',\n",
       " 'restrict',\n",
       " 'restricted',\n",
       " 'restricting',\n",
       " 'restriction',\n",
       " 'restricts',\n",
       " 'reward',\n",
       " 'rewarded',\n",
       " 'rewarding',\n",
       " 'ridicule',\n",
       " 'ridiculed',\n",
       " 'ridicules',\n",
       " 'ridiculing',\n",
       " 'risk',\n",
       " 'risked',\n",
       " 'riskier',\n",
       " 'riskiest',\n",
       " 'riskiness',\n",
       " 'risking',\n",
       " 'risks',\n",
       " 'risky',\n",
       " 'sabotage',\n",
       " 'satisfaction',\n",
       " 'satisfactorily',\n",
       " 'satisfactory',\n",
       " 'satisfied',\n",
       " 'satisfies',\n",
       " 'satisfy',\n",
       " 'satisfying',\n",
       " 'scandalous',\n",
       " 'scandals',\n",
       " 'sentenced',\n",
       " 'sentencing',\n",
       " 'serious',\n",
       " 'seriously',\n",
       " 'seriousness',\n",
       " 'severe',\n",
       " 'severed',\n",
       " 'severely',\n",
       " 'shocked',\n",
       " 'shortage',\n",
       " 'shortages',\n",
       " 'sluggish',\n",
       " 'solves',\n",
       " 'solving',\n",
       " 'spam',\n",
       " 'spammers',\n",
       " 'spamming',\n",
       " 'speculative',\n",
       " 'stable',\n",
       " 'stolen',\n",
       " 'stopped',\n",
       " 'stopping',\n",
       " 'stops',\n",
       " 'strain',\n",
       " 'strained',\n",
       " 'straining',\n",
       " 'strains',\n",
       " 'strength',\n",
       " 'strengthen',\n",
       " 'strengthened',\n",
       " 'strengthening',\n",
       " 'strengthens',\n",
       " 'strengths',\n",
       " 'stress',\n",
       " 'stressed',\n",
       " 'stresses',\n",
       " 'stressful',\n",
       " 'stressing',\n",
       " 'strong',\n",
       " 'stronger',\n",
       " 'strongest',\n",
       " 'strongly',\n",
       " 'succeed',\n",
       " 'succeeded',\n",
       " 'succeeding',\n",
       " 'succeeds',\n",
       " 'success',\n",
       " 'successes',\n",
       " 'successful',\n",
       " 'successfully',\n",
       " 'suffer',\n",
       " 'suffered',\n",
       " 'suffering',\n",
       " 'suffers',\n",
       " 'suing',\n",
       " 'superior',\n",
       " 'sureties',\n",
       " 'surety',\n",
       " 'suspect',\n",
       " 'suspected',\n",
       " 'suspects',\n",
       " 'suspend',\n",
       " 'suspended',\n",
       " 'suspicion',\n",
       " 'suspicions',\n",
       " 'suspicious',\n",
       " 'suspiciously',\n",
       " 'tense',\n",
       " 'threat',\n",
       " 'threaten',\n",
       " 'threatened',\n",
       " 'threatening',\n",
       " 'threatens',\n",
       " 'threats',\n",
       " 'tragedies',\n",
       " 'tragedy',\n",
       " 'tragic',\n",
       " 'tragically',\n",
       " 'traumatic',\n",
       " 'trouble',\n",
       " 'troubled',\n",
       " 'troubles',\n",
       " 'turmoil',\n",
       " 'unacceptable',\n",
       " 'unapproved',\n",
       " 'unattractive',\n",
       " 'unaware',\n",
       " 'uncertain',\n",
       " 'uncertainly',\n",
       " 'uncertainties',\n",
       " 'uncertainty',\n",
       " 'unclear',\n",
       " 'unconfirmed',\n",
       " 'uncontrollable',\n",
       " 'uncontrollably',\n",
       " 'uncontrolled',\n",
       " 'undecided',\n",
       " 'underestimate',\n",
       " 'underestimated',\n",
       " 'underestimates',\n",
       " 'undermine',\n",
       " 'undermined',\n",
       " 'undermines',\n",
       " 'undermining',\n",
       " 'undesirable',\n",
       " 'unemployment',\n",
       " 'unethical',\n",
       " 'unfair',\n",
       " 'unfortunate',\n",
       " 'unfortunately',\n",
       " 'unfriendly',\n",
       " 'unfulfilled',\n",
       " 'unjust',\n",
       " 'unmatched',\n",
       " 'unsatisfied',\n",
       " 'unsavory',\n",
       " 'unsettled',\n",
       " 'unstable',\n",
       " 'unsuccessful',\n",
       " 'unsuccessfully',\n",
       " 'unsure',\n",
       " 'unwanted',\n",
       " 'unwelcome',\n",
       " 'upset',\n",
       " 'urgent',\n",
       " 'vague',\n",
       " 'valuable',\n",
       " 'verdict',\n",
       " 'verdicts',\n",
       " 'vibrant',\n",
       " 'victims',\n",
       " 'violate',\n",
       " 'violated',\n",
       " 'violates',\n",
       " 'violating',\n",
       " 'violation',\n",
       " 'violations',\n",
       " 'violative',\n",
       " 'violator',\n",
       " 'violators',\n",
       " 'violence',\n",
       " 'violent',\n",
       " 'violently',\n",
       " 'vulnerabilities',\n",
       " 'vulnerability',\n",
       " 'vulnerable',\n",
       " 'vulnerably',\n",
       " 'warn',\n",
       " 'warned',\n",
       " 'warning',\n",
       " 'warnings',\n",
       " 'warns',\n",
       " 'wasted',\n",
       " 'wasting',\n",
       " 'weak',\n",
       " 'weaken',\n",
       " 'weakened',\n",
       " 'weakening',\n",
       " 'weakens',\n",
       " 'weaker',\n",
       " 'weakest',\n",
       " 'weakly',\n",
       " 'weakness',\n",
       " 'weaknesses',\n",
       " 'win',\n",
       " 'winner',\n",
       " 'winners',\n",
       " 'winning',\n",
       " 'worries',\n",
       " 'worry',\n",
       " 'worrying',\n",
       " 'worse',\n",
       " 'worsen',\n",
       " 'worsened',\n",
       " 'worsening',\n",
       " 'worsens',\n",
       " 'worst',\n",
       " 'worthless',\n",
       " 'worthy',\n",
       " 'wrong'}"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "ca70563a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Mean Sentiment Rating</th>\n",
       "      <th>Standard Deviation</th>\n",
       "      <th>Raw Human Sentiment Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$:</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.80623</td>\n",
       "      <td>[-1, -1, -1, -1, -3, -1, -3, -1, -2, -1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Token  Mean Sentiment Rating  Standard Deviation  \\\n",
       "0    $:                   -1.5             0.80623   \n",
       "\n",
       "                Raw Human Sentiment Ratings  \n",
       "0  [-1, -1, -1, -1, -3, -1, -3, -1, -2, -1]  "
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader_sentiment.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "a69b3136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Mean Sentiment Rating</th>\n",
       "      <th>Standard Deviation</th>\n",
       "      <th>Raw Human Sentiment Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Token, Mean Sentiment Rating, Standard Deviation, Raw Human Sentiment Ratings]\n",
       "Index: []"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vader data frame \n",
    "\n",
    "vader_sentiment[vader_sentiment[\"Token\"] == common_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "6289c342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         $:\n",
       "1         %)\n",
       "2        %-)\n",
       "3        &-:\n",
       "4         &:\n",
       "        ... \n",
       "7515      }:\n",
       "7516     }:(\n",
       "7517     }:)\n",
       "7518    }:-(\n",
       "7519    }:-)\n",
       "Name: Token, Length: 7520, dtype: object"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in \n",
    "vader_sentiment[\"Token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "04769c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "commondict_msr_v = {}\n",
    "for i in list(vader_sentiment[\"Token\"]):\n",
    "    if i in list(common_words):\n",
    "        commondict_msr_v[i] = float(vader_sentiment[\"Mean Sentiment Rating\"][vader_sentiment[\"Token\"]==i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "4ee44324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.9,\n",
       " -2.0,\n",
       " -1.6,\n",
       " -2.4,\n",
       " -1.7,\n",
       " -1.3,\n",
       " 1.2,\n",
       " 1.5,\n",
       " 1.3,\n",
       " 1.6,\n",
       " -3.2,\n",
       " -2.3,\n",
       " -2.6,\n",
       " -2.0,\n",
       " -3.2,\n",
       " -2.8,\n",
       " -2.5,\n",
       " -2.1,\n",
       " -0.3,\n",
       " -1.4,\n",
       " -1.3,\n",
       " 1.8,\n",
       " 1.9,\n",
       " 1.7,\n",
       " -1.0,\n",
       " -1.3,\n",
       " -0.8,\n",
       " -1.2,\n",
       " -1.4,\n",
       " -0.7,\n",
       " 0.8,\n",
       " 0.1,\n",
       " 1.0,\n",
       " 1.3,\n",
       " 1.0,\n",
       " 1.4,\n",
       " 1.5,\n",
       " 1.9,\n",
       " 1.5,\n",
       " -1.5,\n",
       " -1.0,\n",
       " -0.8,\n",
       " -1.5,\n",
       " -0.8,\n",
       " -1.5,\n",
       " -1.8,\n",
       " -2.5,\n",
       " -1.9,\n",
       " -1.9,\n",
       " -1.2,\n",
       " -1.1,\n",
       " -1.9,\n",
       " -1.3,\n",
       " -1.8,\n",
       " -1.6,\n",
       " -1.7,\n",
       " -1.8,\n",
       " 0.4,\n",
       " -1.4,\n",
       " -1.5,\n",
       " -2.0,\n",
       " -1.5,\n",
       " -1.5,\n",
       " -1.7,\n",
       " -1.4,\n",
       " -2.1,\n",
       " -1.9,\n",
       " -2.8,\n",
       " -2.4,\n",
       " -2.3,\n",
       " -2.5,\n",
       " 1.4,\n",
       " 1.5,\n",
       " 1.3,\n",
       " 1.6,\n",
       " 1.9,\n",
       " 1.8,\n",
       " -2.5,\n",
       " -0.4,\n",
       " -2.6,\n",
       " -0.5,\n",
       " 2.9,\n",
       " 2.7,\n",
       " 1.9,\n",
       " 2.4,\n",
       " 0.4,\n",
       " 1.7,\n",
       " 1.9,\n",
       " 3.2,\n",
       " 1.9,\n",
       " 1.7,\n",
       " 1.5,\n",
       " -1.3,\n",
       " -1.7,\n",
       " -1.7,\n",
       " -1.4,\n",
       " -0.8,\n",
       " 2.8,\n",
       " -1.9,\n",
       " -1.7,\n",
       " -1.4,\n",
       " -1.5,\n",
       " -1.8,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -0.8,\n",
       " -0.9,\n",
       " -1.5,\n",
       " -1.0,\n",
       " -1.4,\n",
       " -3.4,\n",
       " -2.2,\n",
       " -0.4,\n",
       " 0.3,\n",
       " -0.4,\n",
       " 0.3,\n",
       " 0.6,\n",
       " 1.7,\n",
       " 1.7,\n",
       " -1.5,\n",
       " -2.2,\n",
       " -1.1,\n",
       " -1.2,\n",
       " -1.2,\n",
       " -1.5,\n",
       " -1.1,\n",
       " -1.2,\n",
       " 1.2,\n",
       " 1.6,\n",
       " 0.5,\n",
       " 0.1,\n",
       " 1.1,\n",
       " 0.3,\n",
       " 0.2,\n",
       " 0.9,\n",
       " -1.5,\n",
       " -0.7,\n",
       " -1.1,\n",
       " -1.7,\n",
       " -0.8,\n",
       " -1.6,\n",
       " -1.2,\n",
       " -1.7,\n",
       " 2.1,\n",
       " 1.9,\n",
       " 1.8,\n",
       " 2.3,\n",
       " 1.7,\n",
       " 1.3,\n",
       " -1.6,\n",
       " -2.8,\n",
       " -1.9,\n",
       " -2.3,\n",
       " 2.2,\n",
       " -1.3,\n",
       " -1.7,\n",
       " -1.6,\n",
       " -0.7,\n",
       " -1.3,\n",
       " -1.6,\n",
       " -1.5,\n",
       " -0.8,\n",
       " -0.6,\n",
       " -0.9,\n",
       " -0.9,\n",
       " -1.3,\n",
       " -1.3,\n",
       " -0.9,\n",
       " -1.4,\n",
       " -1.2,\n",
       " 0.9,\n",
       " 1.0,\n",
       " -2.4,\n",
       " -0.4,\n",
       " -2.8,\n",
       " 0.2,\n",
       " -1.2,\n",
       " -1.3,\n",
       " -1.3,\n",
       " -1.3,\n",
       " -1.0,\n",
       " -1.3,\n",
       " -1.4,\n",
       " -1.4,\n",
       " -0.8,\n",
       " -0.4,\n",
       " 2.3,\n",
       " 1.9,\n",
       " 1.5,\n",
       " 1.8,\n",
       " 1.6,\n",
       " -2.5,\n",
       " -2.4,\n",
       " -2.7,\n",
       " -3.1,\n",
       " -1.9,\n",
       " -0.9,\n",
       " -1.6,\n",
       " -1.5,\n",
       " -1.4,\n",
       " -1.5,\n",
       " -1.1,\n",
       " -2.2,\n",
       " -1.9,\n",
       " -1.9,\n",
       " -2.3,\n",
       " -2.4,\n",
       " -2.1,\n",
       " -2.0,\n",
       " -2.2,\n",
       " -1.4,\n",
       " -2.0,\n",
       " -1.9,\n",
       " -1.7,\n",
       " -1.9,\n",
       " -1.6,\n",
       " -1.4,\n",
       " -1.9,\n",
       " -2.0,\n",
       " -2.1,\n",
       " -1.6,\n",
       " -1.3,\n",
       " -1.4,\n",
       " -1.9,\n",
       " -2.1,\n",
       " -1.7,\n",
       " 0.1,\n",
       " -1.2,\n",
       " -1.7,\n",
       " 1.7,\n",
       " -2.4,\n",
       " -1.5,\n",
       " -1.9,\n",
       " -1.8,\n",
       " -2.1,\n",
       " -2.8,\n",
       " -1.3,\n",
       " -0.9,\n",
       " 2.9,\n",
       " 2.3,\n",
       " 2.8,\n",
       " 2.7,\n",
       " 1.6,\n",
       " 2.0,\n",
       " -1.9,\n",
       " -1.8,\n",
       " -1.4,\n",
       " -1.4,\n",
       " -2.2,\n",
       " -2.3,\n",
       " -2.2,\n",
       " -1.6,\n",
       " -1.8,\n",
       " -2.1,\n",
       " -2.1,\n",
       " -1.7,\n",
       " -2.0,\n",
       " 1.3,\n",
       " 1.1,\n",
       " -2.5,\n",
       " -2.2,\n",
       " -2.6,\n",
       " -2.6,\n",
       " -2.7,\n",
       " -3.0,\n",
       " -1.8,\n",
       " -1.7,\n",
       " -1.5,\n",
       " -3.1,\n",
       " -3.0,\n",
       " -3.3,\n",
       " -1.8,\n",
       " -1.5,\n",
       " -1.2,\n",
       " -1.7,\n",
       " -1.4,\n",
       " -1.8,\n",
       " -1.7,\n",
       " -1.8,\n",
       " -1.7,\n",
       " -1.6,\n",
       " -1.7,\n",
       " -1.3,\n",
       " -1.4,\n",
       " -1.5,\n",
       " -1.8,\n",
       " -1.3,\n",
       " -0.9,\n",
       " -0.9,\n",
       " -1.4,\n",
       " -1.7,\n",
       " -2.1,\n",
       " -2.2,\n",
       " -1.9,\n",
       " -2.3,\n",
       " -2.0,\n",
       " -1.6,\n",
       " -3.1,\n",
       " -2.6,\n",
       " -2.9,\n",
       " -1.8,\n",
       " -1.7,\n",
       " -1.9,\n",
       " -1.9,\n",
       " -1.9,\n",
       " -2.2,\n",
       " -2.7,\n",
       " -3.0,\n",
       " -2.0,\n",
       " -1.4,\n",
       " -1.6,\n",
       " -2.2,\n",
       " -1.7,\n",
       " -1.4,\n",
       " -1.1,\n",
       " -1.7,\n",
       " -1.8,\n",
       " -1.1,\n",
       " -1.6,\n",
       " -0.9,\n",
       " -1.4,\n",
       " -1.5,\n",
       " -1.4,\n",
       " -1.3,\n",
       " -2.2,\n",
       " -1.6,\n",
       " -1.3,\n",
       " -1.7,\n",
       " -1.1,\n",
       " -1.4,\n",
       " -1.2,\n",
       " -1.4,\n",
       " -1.2,\n",
       " -1.6,\n",
       " -1.0,\n",
       " -1.3,\n",
       " -2.4,\n",
       " -1.8,\n",
       " -1.7,\n",
       " -1.6,\n",
       " -1.4,\n",
       " -1.6,\n",
       " -2.3,\n",
       " -1.9,\n",
       " -1.5,\n",
       " -1.1,\n",
       " -1.4,\n",
       " -1.2,\n",
       " -0.9,\n",
       " 1.0,\n",
       " -1.8,\n",
       " 1.8,\n",
       " 1.4,\n",
       " 1.0,\n",
       " 1.9,\n",
       " 1.6,\n",
       " 1.5,\n",
       " 1.8,\n",
       " 1.7,\n",
       " -1.2,\n",
       " -1.5,\n",
       " -1.7,\n",
       " -1.6,\n",
       " -1.9,\n",
       " -1.7,\n",
       " 1.5,\n",
       " 1.8,\n",
       " 1.9,\n",
       " 2.4,\n",
       " 2.2,\n",
       " 1.9,\n",
       " 1.8,\n",
       " 2.3,\n",
       " 2.4,\n",
       " 2.6,\n",
       " 2.3,\n",
       " 1.9,\n",
       " 2.2,\n",
       " 2.6,\n",
       " -1.8,\n",
       " -1.7,\n",
       " -1.4,\n",
       " -2.0,\n",
       " -0.6,\n",
       " -0.4,\n",
       " -0.6,\n",
       " -0.7,\n",
       " 3.1,\n",
       " 2.7,\n",
       " 2.5,\n",
       " 2.5,\n",
       " 1.4,\n",
       " 2.2,\n",
       " 2.2,\n",
       " 0.5,\n",
       " 1.8,\n",
       " 1.8,\n",
       " 1.6,\n",
       " 1.0,\n",
       " -0.4,\n",
       " -2.0,\n",
       " -1.9,\n",
       " -1.4,\n",
       " -0.6,\n",
       " -0.3,\n",
       " -0.5,\n",
       " -1.1,\n",
       " -2.5,\n",
       " -2.3,\n",
       " -2.3,\n",
       " -2.2,\n",
       " -1.8,\n",
       " -2.3,\n",
       " -2.0,\n",
       " -1.6,\n",
       " -2.0,\n",
       " 2.6,\n",
       " -2.9,\n",
       " -3.5,\n",
       " -3.2,\n",
       " -1.7,\n",
       " -1.4,\n",
       " -2.1,\n",
       " -1.3,\n",
       " 2.1,\n",
       " 1.6,\n",
       " 1.8,\n",
       " 1.8,\n",
       " 2.0,\n",
       " 1.8,\n",
       " -2.2,\n",
       " -1.8,\n",
       " -2.5,\n",
       " -2.5,\n",
       " -2.6,\n",
       " -1.4,\n",
       " -2.1,\n",
       " -1.3,\n",
       " -1.8,\n",
       " -1.9,\n",
       " -1.3,\n",
       " -2.0,\n",
       " -0.5,\n",
       " -2.4,\n",
       " -2.8,\n",
       " -2.3,\n",
       " -2.3,\n",
       " -2.2,\n",
       " 2.2,\n",
       " -2.0,\n",
       " -2.4,\n",
       " -1.9,\n",
       " -1.9,\n",
       " -2.0,\n",
       " -2.1,\n",
       " -2.0,\n",
       " 2.4,\n",
       " 1.6,\n",
       " 1.8,\n",
       " 1.4,\n",
       " 1.9,\n",
       " 3.2,\n",
       " -2.1,\n",
       " -1.5,\n",
       " -0.9,\n",
       " -1.8,\n",
       " 3.2,\n",
       " 2.6,\n",
       " 2.6,\n",
       " 2.7,\n",
       " -2.2,\n",
       " -2.5,\n",
       " -2.5,\n",
       " -2.5,\n",
       " -1.3,\n",
       " -2.5,\n",
       " -2.1,\n",
       " -2.6,\n",
       " -2.6,\n",
       " -2.2,\n",
       " -1.9,\n",
       " -2.2,\n",
       " -2.9,\n",
       " -1.7,\n",
       " 2.2,\n",
       " 2.8,\n",
       " 2.3,\n",
       " 2.3,\n",
       " -1.6,\n",
       " -2.5,\n",
       " -2.4,\n",
       " -1.7,\n",
       " 2.4,\n",
       " -1.5,\n",
       " -1.3,\n",
       " -1.1,\n",
       " -1.7,\n",
       " -1.8,\n",
       " -2.6,\n",
       " -2.0,\n",
       " -1.2,\n",
       " -0.3,\n",
       " -0.4,\n",
       " -0.4,\n",
       " 1.9,\n",
       " 2.1,\n",
       " 2.1,\n",
       " 2.5,\n",
       " 2.3,\n",
       " 2.0,\n",
       " 1.9,\n",
       " 2.1,\n",
       " 2.0,\n",
       " 1.3,\n",
       " 1.8,\n",
       " 1.8,\n",
       " -1.7,\n",
       " -1.0,\n",
       " -1.7,\n",
       " -1.7,\n",
       " -1.7,\n",
       " -1.0,\n",
       " -1.6,\n",
       " -1.9,\n",
       " -2.3,\n",
       " -2.1,\n",
       " -1.5,\n",
       " -1.4,\n",
       " -0.5,\n",
       " -1.3,\n",
       " -1.3,\n",
       " -1.7,\n",
       " 1.9,\n",
       " -2.1,\n",
       " -1.6,\n",
       " -0.4,\n",
       " -0.4,\n",
       " -0.9,\n",
       " -1.7,\n",
       " -1.8,\n",
       " 2.2,\n",
       " 2.0,\n",
       " 1.6,\n",
       " 1.9,\n",
       " -1.8,\n",
       " -0.9,\n",
       " 2.4,\n",
       " 2.3,\n",
       " 1.6,\n",
       " -1.6,\n",
       " -1.4,\n",
       " -1.2,\n",
       " -1.2,\n",
       " -1.5,\n",
       " -1.7,\n",
       " -1.3,\n",
       " -1.8,\n",
       " -0.8,\n",
       " 2.4,\n",
       " -1.3,\n",
       " -1.4,\n",
       " -1.2,\n",
       " -1.1,\n",
       " -1.5,\n",
       " -0.9,\n",
       " -0.6,\n",
       " 0.5,\n",
       " 0.4,\n",
       " -2.1,\n",
       " -1.2,\n",
       " -0.8,\n",
       " -0.8,\n",
       " -1.7,\n",
       " -1.3,\n",
       " -1.6,\n",
       " -1.3,\n",
       " -1.7,\n",
       " -1.3,\n",
       " -1.6,\n",
       " 2.1,\n",
       " -2.4,\n",
       " 0.3,\n",
       " -1.6,\n",
       " -1.5,\n",
       " -1.2,\n",
       " 2.1,\n",
       " -1.5,\n",
       " -1.3,\n",
       " -1.6,\n",
       " -1.3,\n",
       " -1.7,\n",
       " -2.0,\n",
       " -0.6,\n",
       " -1.2,\n",
       " -0.9,\n",
       " -1.4,\n",
       " -1.5,\n",
       " -1.2,\n",
       " -1.5,\n",
       " -1.1,\n",
       " -1.5,\n",
       " -1.8,\n",
       " -1.4,\n",
       " -0.8,\n",
       " -0.9,\n",
       " -1.1,\n",
       " -0.5,\n",
       " -2.7,\n",
       " -2.0,\n",
       " -2.4,\n",
       " -2.0,\n",
       " -1.7,\n",
       " -2.2,\n",
       " -2.8,\n",
       " -1.2,\n",
       " -1.5,\n",
       " -1.6,\n",
       " -1.2,\n",
       " -1.4,\n",
       " -1.2,\n",
       " -1.0,\n",
       " -1.5,\n",
       " -1.5,\n",
       " -2.3,\n",
       " -2.0,\n",
       " -1.0,\n",
       " -0.1,\n",
       " 0.9,\n",
       " 1.6,\n",
       " 1.8,\n",
       " 1.3,\n",
       " -1.5,\n",
       " -0.1,\n",
       " -1.1,\n",
       " -0.7,\n",
       " -2.3,\n",
       " -1.9,\n",
       " -2.0,\n",
       " 2.7,\n",
       " 2.7,\n",
       " 3.2,\n",
       " 1.6,\n",
       " -1.7,\n",
       " -1.9,\n",
       " -0.8,\n",
       " 2.3,\n",
       " 2.1,\n",
       " 1.9,\n",
       " 2.7,\n",
       " -2.1,\n",
       " 1.8,\n",
       " 2.1,\n",
       " 2.6,\n",
       " 2.4,\n",
       " -0.9,\n",
       " -0.8,\n",
       " -1.1,\n",
       " -0.5,\n",
       " -2.3,\n",
       " -1.9,\n",
       " -1.8,\n",
       " -2.6,\n",
       " -1.8,\n",
       " 0.1,\n",
       " 0.1,\n",
       " -0.1,\n",
       " 0.3,\n",
       " 1.8,\n",
       " -1.7,\n",
       " -1.9,\n",
       " -1.8,\n",
       " -1.7,\n",
       " 1.1,\n",
       " 1.9,\n",
       " 1.6,\n",
       " 1.8,\n",
       " -1.7,\n",
       " -1.6,\n",
       " -1.8,\n",
       " -2.2,\n",
       " 2.1,\n",
       " -1.0,\n",
       " -0.5,\n",
       " -0.9,\n",
       " -1.8,\n",
       " -0.9,\n",
       " -1.7,\n",
       " -1.1,\n",
       " -1.3,\n",
       " -0.8,\n",
       " -1.9,\n",
       " -2.0,\n",
       " -2.1,\n",
       " -2.6,\n",
       " -2.2,\n",
       " -1.8,\n",
       " -2.3,\n",
       " -1.2,\n",
       " -0.4,\n",
       " -0.4,\n",
       " -1.8,\n",
       " -1.7,\n",
       " -1.2,\n",
       " -1.2,\n",
       " -1.7,\n",
       " -1.7,\n",
       " -2.3,\n",
       " -2.0,\n",
       " -2.5,\n",
       " -2.1,\n",
       " -2.2,\n",
       " -1.4,\n",
       " -1.0,\n",
       " -1.4,\n",
       " -1.2,\n",
       " -1.2,\n",
       " -1.0,\n",
       " -0.9,\n",
       " -1.3,\n",
       " 1.6,\n",
       " -1.6,\n",
       " -1.6,\n",
       " -1.6,\n",
       " -1.1,\n",
       " -1.3,\n",
       " 2.7,\n",
       " 2.2,\n",
       " 2.4,\n",
       " -2.0,\n",
       " -1.5,\n",
       " -1.8,\n",
       " -1.8,\n",
       " -1.1,\n",
       " -0.9,\n",
       " -1.4,\n",
       " -1.5,\n",
       " -1.3,\n",
       " -1.3,\n",
       " -1.1,\n",
       " -0.8,\n",
       " -2.4,\n",
       " 1.9,\n",
       " 1.6,\n",
       " 1.5,\n",
       " 1.8,\n",
       " 1.8,\n",
       " 2.0,\n",
       " 2.0,\n",
       " -2.4,\n",
       " -2.2,\n",
       " -0.1,\n",
       " -0.6,\n",
       " -0.3,\n",
       " -0.7,\n",
       " -0.2,\n",
       " -1.6,\n",
       " -1.5,\n",
       " -2.0,\n",
       " -1.3,\n",
       " -1.0,\n",
       " -0.6,\n",
       " -1.7,\n",
       " 1.1,\n",
       " 1.4,\n",
       " -1.5,\n",
       " -1.6,\n",
       " -2.1,\n",
       " 0.4,\n",
       " 1.2,\n",
       " -2.2,\n",
       " -0.9,\n",
       " -0.6,\n",
       " -0.6,\n",
       " -0.2,\n",
       " -1.7,\n",
       " -1.3,\n",
       " -1.2,\n",
       " 2.2,\n",
       " 1.3,\n",
       " 1.8,\n",
       " 2.2,\n",
       " 2.0,\n",
       " 1.7,\n",
       " -1.8,\n",
       " -1.4,\n",
       " -2.0,\n",
       " -2.3,\n",
       " -1.5,\n",
       " 2.3,\n",
       " 1.6,\n",
       " 1.9,\n",
       " 1.1,\n",
       " 2.2,\n",
       " 1.8,\n",
       " 2.2,\n",
       " 2.2,\n",
       " 2.7,\n",
       " 2.6,\n",
       " 2.8,\n",
       " 2.2,\n",
       " -2.5,\n",
       " -2.2,\n",
       " -2.1,\n",
       " -2.1,\n",
       " -1.1,\n",
       " 2.5,\n",
       " 1.3,\n",
       " 1.0,\n",
       " -1.2,\n",
       " -0.9,\n",
       " -1.4,\n",
       " -1.3,\n",
       " -2.1,\n",
       " -1.6,\n",
       " -1.5,\n",
       " -1.5,\n",
       " -1.7,\n",
       " -1.4,\n",
       " -2.4,\n",
       " -1.6,\n",
       " -2.0,\n",
       " -2.4,\n",
       " -1.6,\n",
       " -1.8,\n",
       " -1.9,\n",
       " -3.4,\n",
       " -2.0,\n",
       " -2.7,\n",
       " -2.7,\n",
       " -1.7,\n",
       " -2.0,\n",
       " -2.0,\n",
       " -1.5,\n",
       " -2.0,\n",
       " -1.4,\n",
       " -1.9,\n",
       " -0.8,\n",
       " -1.2,\n",
       " -1.4,\n",
       " -1.4,\n",
       " -1.4,\n",
       " -1.0,\n",
       " -0.5,\n",
       " -1.5,\n",
       " -1.5,\n",
       " -1.0,\n",
       " -0.9,\n",
       " -1.2,\n",
       " -1.1,\n",
       " -1.1,\n",
       " -1.2,\n",
       " -1.5,\n",
       " -1.4,\n",
       " -1.5,\n",
       " -1.9,\n",
       " -1.9,\n",
       " -2.3,\n",
       " -2.1,\n",
       " -2.0,\n",
       " -1.4,\n",
       " -1.5,\n",
       " -1.8,\n",
       " -2.3,\n",
       " -0.3,\n",
       " -1.7,\n",
       " -1.9,\n",
       " -1.3,\n",
       " -1.5,\n",
       " -1.5,\n",
       " -1.7,\n",
       " -1.0,\n",
       " -0.9,\n",
       " -1.7,\n",
       " -1.6,\n",
       " 0.8,\n",
       " -0.4,\n",
       " 2.1,\n",
       " 0.6,\n",
       " 0.3,\n",
       " 2.4,\n",
       " -1.3,\n",
       " -2.2,\n",
       " -2.4,\n",
       " -2.3,\n",
       " -2.5,\n",
       " -2.2,\n",
       " -2.4,\n",
       " -2.4,\n",
       " -2.4,\n",
       " -1.9,\n",
       " -3.1,\n",
       " -2.9,\n",
       " -2.8,\n",
       " -0.6,\n",
       " -0.9,\n",
       " -0.9,\n",
       " -1.2,\n",
       " -0.4,\n",
       " -1.1,\n",
       " -1.4,\n",
       " -1.2,\n",
       " -0.4,\n",
       " -2.2,\n",
       " -1.7,\n",
       " -1.9,\n",
       " -1.8,\n",
       " -1.3,\n",
       " -1.3,\n",
       " -1.3,\n",
       " -1.9,\n",
       " -2.3,\n",
       " -1.8,\n",
       " -1.8,\n",
       " -1.5,\n",
       " 2.8,\n",
       " 2.8,\n",
       " 2.1,\n",
       " 2.4,\n",
       " -1.8,\n",
       " -1.9,\n",
       " -1.4,\n",
       " -2.1,\n",
       " -2.3,\n",
       " -1.9,\n",
       " -2.0,\n",
       " -2.1,\n",
       " -3.1,\n",
       " -1.9,\n",
       " 1.9,\n",
       " -2.1]"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(commondict_msr_v.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "44075474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Seq_num</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Word Proportion</th>\n",
       "      <th>Average Proportion</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Doc Count</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Uncertainty</th>\n",
       "      <th>Litigious</th>\n",
       "      <th>Strong_Modal</th>\n",
       "      <th>Weak_Modal</th>\n",
       "      <th>Constraining</th>\n",
       "      <th>Syllables</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AARDVARK</td>\n",
       "      <td>1</td>\n",
       "      <td>354</td>\n",
       "      <td>1.550080e-08</td>\n",
       "      <td>1.422600e-08</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word  Seq_num  Word Count  Word Proportion  Average Proportion  \\\n",
       "0  AARDVARK        1         354     1.550080e-08        1.422600e-08   \n",
       "\n",
       "    Std Dev  Doc Count  Negative  Positive  Uncertainty  Litigious  \\\n",
       "0  0.000004         99         0         0            0          0   \n",
       "\n",
       "   Strong_Modal  Weak_Modal  Constraining  Syllables     Source  \n",
       "0             0           0             0          2  12of12inf  "
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dict.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "90390d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the negative list \n",
    "commondict_lm_neg = {}\n",
    "for i in list(lm_dict[\"Word\"].str.lower()):\n",
    "    #i = i.lower()\n",
    "    if i in list(common_words):\n",
    "        i = i.upper()\n",
    "        commondict_lm_neg[i] = float(lm_dict[\"Negative\"][lm_dict[\"Word\"]==i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "239daf0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "930"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(commondict_lm_neg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "0cd3193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the positive list \n",
    "commondict_lm_pos = {}\n",
    "for i in list(lm_dict[\"Word\"].str.lower()):\n",
    "    #i = i.lower()\n",
    "    if i in list(common_words):\n",
    "        i = i.upper()\n",
    "        commondict_lm_pos[i] = float(lm_dict[\"Positive\"][lm_dict[\"Word\"]==i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "ece4102b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "930"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(commondict_lm_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "c0226542",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the Uncertainty  list \n",
    "commondict_lm_unc = {}\n",
    "for i in list(lm_dict[\"Word\"].str.lower()):\n",
    "    #i = i.lower()\n",
    "    if i in list(common_words):\n",
    "        i = i.upper()\n",
    "        commondict_lm_unc[i] = float(lm_dict[\"Uncertainty\"][lm_dict[\"Word\"]==i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "d46c0966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2009.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2009.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2009.0, 2009.0, 2009.0, 2009.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2009.0, 2009.0, 2009.0, 2009.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2009.0, 2009.0, 2009.0, 2009.0, 2009.0, 2009.0, 2009.0, 2009.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2009.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2009.0, 2009.0, 2009.0, 2009.0, 2009.0, 2009.0, 0.0, 0.0, 0.0, 2009.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2009.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2012.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(commondict_lm_unc).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc647b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
